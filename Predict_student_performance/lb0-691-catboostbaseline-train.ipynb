{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "papermill": {
     "duration": 85.86438,
     "end_time": "2023-03-10T11:59:45.753511",
     "exception": false,
     "start_time": "2023-03-10T11:58:19.889131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "import math\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('predict-student-performance-from-game-play/train_labels.csv')\n",
    "targets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]))\n",
    "targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"session_id\": pl.Int64,\n",
    "          \"elapsed_time\": pl.Int64,\n",
    "          \"event_name\": pl.Categorical,\n",
    "          \"name\": pl.Categorical,\n",
    "          \"level\": pl.Int8,\n",
    "          \"page\": pl.Float32,\n",
    "          \"room_coor_x\": pl.Float32,\n",
    "          \"room_coor_y\": pl.Float32,\n",
    "          \"screen_coor_x\": pl.Float32,\n",
    "          \"screen_coor_y\": pl.Float32,\n",
    "          \"hover_duration\": pl.Float32,\n",
    "          \"text\": pl.Categorical,\n",
    "          \"fqid\": pl.Categorical,\n",
    "          \"room_fqid\": pl.Categorical,\n",
    "          \"text_fqid\": pl.Categorical,\n",
    "          \"fullscreen\": pl.Int8,\n",
    "          \"hq\": pl.Int8,\n",
    "          \"music\": pl.Int8,\n",
    "          \"level_group\": pl.Categorical\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "\n",
    "    pl.col(\"page\").cast(pl.Float32),\n",
    "    (\n",
    "        (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1)) \n",
    "         .fill_null(0)\n",
    "         .clip(0, 1e9)\n",
    "         .over([\"session_id\", \"level\"])\n",
    "         .alias(\"elapsed_time_diff\")\n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_x\") - pl.col(\"screen_coor_x\").shift(1)) \n",
    "         .abs()\n",
    "         .over([\"session_id\", \"level\"])\n",
    "        .alias(\"location_x_diff\") \n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_y\") - pl.col(\"screen_coor_y\").shift(1)) \n",
    "         .abs()\n",
    "         .over([\"session_id\", \"level\"])\n",
    "        .alias(\"location_y_diff\") \n",
    "    )\n",
    "]\n",
    "df = (pl.read_csv('predict-student-performance-from-game-play/train.csv',dtypes=dtypes)\n",
    "      .drop([\"fullscreen\", \"hq\", \"music\"])\n",
    "      .with_columns(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage_pl(df, name):\n",
    "    \"\"\" Reduce memory usage by polars dataframe {df} with name {name} by changing its data types.\n",
    "        Original pandas version of this function: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 \"\"\"\n",
    "    print(f\"Memory usage of dataframe {name} is {round(df.estimated_size('mb'), 2)} MB\")\n",
    "    Numeric_Int_types = [pl.Int8,pl.Int16,pl.Int32,pl.Int64]\n",
    "    Numeric_Float_types = [pl.Float32,pl.Float64]    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        if col_type in Numeric_Int_types:\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df = df.with_columns(df[col].cast(pl.Int8))\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df = df.with_columns(df[col].cast(pl.Int16))\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df = df.with_columns(df[col].cast(pl.Int32))\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df = df.with_columns(df[col].cast(pl.Int64))\n",
    "        elif col_type in Numeric_Float_types:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df = df.with_columns(df[col].cast(pl.Float32))\n",
    "            else:\n",
    "                pass\n",
    "        elif col_type == pl.Utf8:\n",
    "            df = df.with_columns(df[col].cast(pl.Categorical))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    print(f\"Memory usage of dataframe {name} became {round(df.estimated_size('mb'), 2)} MB\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe train_subset is 2366.85 MB\n",
      "Memory usage of dataframe train_subset became 2015.75 MB\n"
     ]
    }
   ],
   "source": [
    "df = reduce_memory_usage_pl(df, \"train_subset\")\n",
    "df1 = df.filter(pl.col(\"level_group\") == '0-4')\n",
    "df2 = df.filter(pl.col(\"level_group\") == '5-12')\n",
    "df3 = df.filter(pl.col(\"level_group\") == '13-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid']\n",
    "NUMS = ['page', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y',\n",
    "        'hover_duration', 'elapsed_time_diff']\n",
    "fqid_lists = ['worker', 'archivist', 'gramps', 'wells', 'toentry', 'confrontation', 'crane_ranger', 'groupconvo', 'flag_girl', 'tomap', 'tostacks', 'tobasement', 'archivist_glasses', 'boss', 'journals', 'seescratches', 'groupconvo_flag', 'cs', 'teddy', 'expert', 'businesscards', 'ch3start', 'tunic.historicalsociety', 'tofrontdesk', 'savedteddy', 'plaque', 'glasses', 'tunic.drycleaner', 'reader_flag', 'tunic.library', 'tracks', 'tunic.capitol_2', 'trigger_scarf', 'reader', 'directory', 'tunic.capitol_1', 'journals.pic_0.next', 'unlockdoor', 'tunic', 'what_happened', 'tunic.kohlcenter', 'tunic.humanecology', 'colorbook', 'logbook', 'businesscards.card_0.next', 'journals.hub.topics', 'logbook.page.bingo', 'journals.pic_1.next', 'journals_flag', 'reader.paper0.next', 'tracks.hub.deer', 'reader_flag.paper0.next', 'trigger_coffee', 'wellsbadge', 'journals.pic_2.next', 'tomicrofiche', 'journals_flag.pic_0.bingo', 'plaque.face.date', 'notebook', 'tocloset_dirty', 'businesscards.card_bingo.bingo', 'businesscards.card_1.next', 'tunic.wildlife', 'tunic.hub.slip', 'tocage', 'journals.pic_2.bingo', 'tocollectionflag', 'tocollection', 'chap4_finale_c', 'chap2_finale_c', 'lockeddoor', 'journals_flag.hub.topics', 'tunic.capitol_0', 'reader_flag.paper2.bingo', 'photo', 'tunic.flaghouse', 'reader.paper1.next', 'directory.closeup.archivist', 'intro', 'businesscards.card_bingo.next', 'reader.paper2.bingo', 'retirement_letter', 'remove_cup', 'journals_flag.pic_0.next', 'magnify', 'coffee', 'key', 'togrampa', 'reader_flag.paper1.next', 'janitor', 'tohallway', 'chap1_finale', 'report', 'outtolunch', 'journals_flag.hub.topics_old', 'journals_flag.pic_1.next', 'reader.paper2.next', 'chap1_finale_c', 'reader_flag.paper2.next', 'door_block_talk', 'journals_flag.pic_1.bingo', 'journals_flag.pic_2.next', 'journals_flag.pic_2.bingo', 'block_magnify', 'reader.paper0.prev', 'block', 'reader_flag.paper0.prev', 'block_0', 'door_block_clean', 'reader.paper2.prev', 'reader.paper1.prev', 'doorblock', 'tocloset', 'reader_flag.paper2.prev', 'reader_flag.paper1.prev', 'block_tomap2', 'journals_flag.pic_0_old.next', 'journals_flag.pic_1_old.next', 'block_tocollection', 'block_nelson', 'journals_flag.pic_2_old.next', 'block_tomap1', 'block_badge', 'need_glasses', 'block_badge_2', 'fox', 'block_1']\n",
    "DIALOGS = ['that', 'this', 'it', 'you', 'flag', 'can', 'and', 'is', 'the', 'to']\n",
    "name_feature = ['basic', 'undefined', 'close', 'open', 'prev', 'next']\n",
    "event_name_feature = ['cutscene_click', 'person_click', 'navigate_click',\n",
    "       'observation_click', 'notification_click', 'object_click',\n",
    "       'object_hover', 'map_hover', 'map_click', 'checkpoint',\n",
    "       'notebook_click']\n",
    "text_lists = ['tunic.historicalsociety.cage.confrontation', 'tunic.wildlife.center.crane_ranger.crane', 'tunic.historicalsociety.frontdesk.archivist.newspaper', 'tunic.historicalsociety.entry.groupconvo', 'tunic.wildlife.center.wells.nodeer', 'tunic.historicalsociety.frontdesk.archivist.have_glass', 'tunic.drycleaner.frontdesk.worker.hub', 'tunic.historicalsociety.closet_dirty.gramps.news', 'tunic.humanecology.frontdesk.worker.intro', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'tunic.historicalsociety.basement.seescratches', 'tunic.historicalsociety.collection.cs', 'tunic.flaghouse.entry.flag_girl.hello', 'tunic.historicalsociety.collection.gramps.found', 'tunic.historicalsociety.basement.ch3start', 'tunic.historicalsociety.entry.groupconvo_flag', 'tunic.library.frontdesk.worker.hello', 'tunic.library.frontdesk.worker.wells', 'tunic.historicalsociety.collection_flag.gramps.flag', 'tunic.historicalsociety.basement.savedteddy', 'tunic.library.frontdesk.worker.nelson', 'tunic.wildlife.center.expert.removed_cup', 'tunic.library.frontdesk.worker.flag', 'tunic.historicalsociety.frontdesk.archivist.hello', 'tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'tunic.historicalsociety.entry.boss.flag', 'tunic.flaghouse.entry.flag_girl.symbol', 'tunic.historicalsociety.closet_dirty.trigger_scarf', 'tunic.drycleaner.frontdesk.worker.done', 'tunic.historicalsociety.closet_dirty.what_happened', 'tunic.wildlife.center.wells.animals', 'tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'tunic.historicalsociety.cage.glasses.afterteddy', 'tunic.historicalsociety.cage.teddy.trapped', 'tunic.historicalsociety.cage.unlockdoor', 'tunic.historicalsociety.stacks.journals.pic_2.bingo', 'tunic.historicalsociety.entry.wells.flag', 'tunic.humanecology.frontdesk.worker.badger', 'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'tunic.historicalsociety.closet.intro', 'tunic.historicalsociety.closet.retirement_letter.hub', 'tunic.historicalsociety.entry.directory.closeup.archivist', 'tunic.historicalsociety.collection.tunic.slip', 'tunic.kohlcenter.halloffame.plaque.face.date', 'tunic.historicalsociety.closet_dirty.trigger_coffee', 'tunic.drycleaner.frontdesk.logbook.page.bingo', 'tunic.library.microfiche.reader.paper2.bingo', 'tunic.kohlcenter.halloffame.togrampa', 'tunic.capitol_2.hall.boss.haveyougotit', 'tunic.wildlife.center.wells.nodeer_recap', 'tunic.historicalsociety.cage.glasses.beforeteddy', 'tunic.historicalsociety.closet_dirty.gramps.helpclean', 'tunic.wildlife.center.expert.recap', 'tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'tunic.historicalsociety.cage.lockeddoor', 'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'tunic.historicalsociety.collection.gramps.lost', 'tunic.historicalsociety.closet.notebook', 'tunic.historicalsociety.frontdesk.magnify', 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'tunic.wildlife.center.remove_cup', 'tunic.library.frontdesk.wellsbadge.hub', 'tunic.wildlife.center.tracks.hub.deer', 'tunic.historicalsociety.frontdesk.key', 'tunic.library.microfiche.reader_flag.paper2.bingo', 'tunic.flaghouse.entry.colorbook', 'tunic.wildlife.center.coffee', 'tunic.capitol_1.hall.boss.haveyougotit', 'tunic.historicalsociety.basement.janitor', 'tunic.historicalsociety.collection_flag.gramps.recap', 'tunic.wildlife.center.wells.animals2', 'tunic.flaghouse.entry.flag_girl.symbol_recap', 'tunic.historicalsociety.closet_dirty.photo', 'tunic.historicalsociety.stacks.outtolunch', 'tunic.library.frontdesk.worker.wells_recap', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'tunic.capitol_0.hall.boss.talktogramps', 'tunic.historicalsociety.closet.photo', 'tunic.historicalsociety.collection.tunic', 'tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'tunic.historicalsociety.closet_dirty.gramps.archivist', 'tunic.historicalsociety.closet_dirty.door_block_talk', 'tunic.historicalsociety.entry.boss.flag_recap', 'tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'tunic.historicalsociety.entry.wells.talktogramps', 'tunic.historicalsociety.frontdesk.block_magnify', 'tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'tunic.historicalsociety.closet_dirty.gramps.nothing', 'tunic.historicalsociety.closet_dirty.door_block_clean', 'tunic.capitol_1.hall.boss.writeitup', 'tunic.library.frontdesk.worker.nelson_recap', 'tunic.library.frontdesk.worker.hello_short', 'tunic.historicalsociety.stacks.block', 'tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'tunic.historicalsociety.entry.boss.talktogramps', 'tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'tunic.historicalsociety.entry.wells.flag_recap', 'tunic.drycleaner.frontdesk.worker.done2', 'tunic.library.frontdesk.worker.flag_recap', 'tunic.humanecology.frontdesk.block_0', 'tunic.library.frontdesk.worker.preflag', 'tunic.historicalsociety.basement.gramps.seeyalater', 'tunic.flaghouse.entry.flag_girl.hello_recap', 'tunic.historicalsociety.closet.doorblock', 'tunic.drycleaner.frontdesk.worker.takealook', 'tunic.historicalsociety.basement.gramps.whatdo', 'tunic.library.frontdesk.worker.droppedbadge', 'tunic.historicalsociety.entry.block_tomap2', 'tunic.library.frontdesk.block_nelson', 'tunic.library.microfiche.block_0', 'tunic.historicalsociety.entry.block_tocollection', 'tunic.historicalsociety.entry.block_tomap1', 'tunic.historicalsociety.collection.gramps.look_0', 'tunic.library.frontdesk.block_badge', 'tunic.historicalsociety.cage.need_glasses', 'tunic.library.frontdesk.block_badge_2', 'tunic.kohlcenter.halloffame.block_0', 'tunic.capitol_0.hall.chap1_finale_c', 'tunic.capitol_1.hall.chap2_finale_c', 'tunic.capitol_2.hall.chap4_finale_c', 'tunic.wildlife.center.fox.concern', 'tunic.drycleaner.frontdesk.block_0', 'tunic.historicalsociety.entry.gramps.hub', 'tunic.humanecology.frontdesk.block_1', 'tunic.drycleaner.frontdesk.block_1']\n",
    "room_lists = ['tunic.historicalsociety.entry', 'tunic.wildlife.center', 'tunic.historicalsociety.cage', 'tunic.library.frontdesk', 'tunic.historicalsociety.frontdesk', 'tunic.historicalsociety.stacks', 'tunic.historicalsociety.closet_dirty', 'tunic.humanecology.frontdesk', 'tunic.historicalsociety.basement', 'tunic.kohlcenter.halloffame', 'tunic.library.microfiche', 'tunic.drycleaner.frontdesk', 'tunic.historicalsociety.collection', 'tunic.historicalsociety.closet', 'tunic.flaghouse.entry', 'tunic.historicalsociety.collection_flag', 'tunic.capitol_1.hall', 'tunic.capitol_0.hall', 'tunic.capitol_2.hall']\n",
    "\n",
    "\n",
    "LEVELS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
    "level_groups = [\"0-4\", \"5-12\", \"13-22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(x, grp, use_extra, feature_suffix):\n",
    "    aggs = [\n",
    "        pl.col(\"index\").count().alias(f\"session_number_{feature_suffix}\"),\n",
    "\n",
    "        *[pl.col('index').filter(pl.col('text').cast(pl.Utf8, strict=False).str.contains(c)).count().alias(f'word_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').cast(pl.Utf8, strict=False).str.contains(c))).mean().alias(f'word_mean_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').cast(pl.Utf8, strict=False).str.contains(c))).std().alias(f'word_std_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col('text').cast(pl.Utf8, strict=False).str.contains(c)).max().alias(f'word_max_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col('text').cast(pl.Utf8, strict=False).str.contains(c)).sum().alias(f'word_sum_{c}') for c in DIALOGS],\n",
    "\n",
    "        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique_{feature_suffix}\") for c in CATS],\n",
    "\n",
    "        *[pl.col(c).quantile(0.1, \"nearest\").alias(f\"{c}_quantile1_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.2, \"nearest\").alias(f\"{c}_quantile2_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.4, \"nearest\").alias(f\"{c}_quantile4_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.6, \"nearest\").alias(f\"{c}_quantile6_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.8, \"nearest\").alias(f\"{c}_quantile8_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.9, \"nearest\").alias(f\"{c}_quantile9_{feature_suffix}\") for c in NUMS],\n",
    "\n",
    "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).std().alias(f\"{c}_std_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in NUMS],\n",
    "\n",
    "        *[pl.col(\"fqid\").filter(pl.col(\"fqid\") == c).count().alias(f\"{c}_fqid_counts{feature_suffix}\")\n",
    "          for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in fqid_lists],\n",
    "\n",
    "        *[pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == c).count().alias(f\"{c}_text_fqid_counts{feature_suffix}\")\n",
    "          for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in text_lists],\n",
    "\n",
    "        *[pl.col(\"room_fqid\").filter(pl.col(\"room_fqid\") == c).count().alias(f\"{c}_room_fqid_counts{feature_suffix}\")\n",
    "          for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in room_lists],\n",
    "\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.1, \"nearest\").alias(\n",
    "            f\"{c}_ET_quantile1_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.2, \"nearest\").alias(\n",
    "            f\"{c}_ET_quantile2_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.4, \"nearest\").alias(\n",
    "            f\"{c}_ET_quantile4_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.6, \"nearest\").alias(\n",
    "            f\"{c}_ET_quantile6_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.8, \"nearest\").alias(\n",
    "            f\"{c}_ET_quantile8_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).quantile(0.9, \"nearest\").alias(\n",
    "            f\"{c}_ET_quantile9_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"event_name\").filter(pl.col(\"event_name\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\")\n",
    "          for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\")\n",
    "          for\n",
    "          c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in event_name_feature],\n",
    "\n",
    "        *[pl.col(\"name\").filter(pl.col(\"name\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
    "          name_feature],\n",
    "\n",
    "        *[pl.col(\"level\").filter(pl.col(\"level\") == c).count().alias(f\"{c}_LEVEL_count{feature_suffix}\") for c in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c\n",
    "          in\n",
    "          LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in\n",
    "          LEVELS],\n",
    "\n",
    "        *[pl.col(\"level_group\").filter(pl.col(\"level_group\") == c).count().alias(\n",
    "            f\"{c}_LEVEL_group_count{feature_suffix}\") for c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for\n",
    "          c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\")\n",
    "          for c in\n",
    "          level_groups],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level_group\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for\n",
    "          c in\n",
    "          level_groups],\n",
    "\n",
    "        *[pl.col(\"index\").filter((pl.col(\"level\") == c) & (pl.col('room_fqid') == d)).count().alias(\n",
    "            f\"{c}{d}_level_room_count{feature_suffix}\") for c in LEVELS for d in room_lists],\n",
    "\n",
    "    ]\n",
    "\n",
    "    df = x.groupby(['session_id'], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "\n",
    "    if use_extra:\n",
    "        if grp == '5-12':\n",
    "            aggs = [\n",
    "                pl.col(\"elapsed_time\").filter((pl.col(\"text\") == \"Here's the log book.\")\n",
    "                                              | (pl.col(\"fqid\") == 'logbook.page.bingo'))\n",
    "                    .apply(lambda s: s.max() - s.min()).alias(\"logbook_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(\n",
    "                    (pl.col(\"text\") == \"Here's the log book.\") | (pl.col(\"fqid\") == 'logbook.page.bingo')).apply(\n",
    "                    lambda s: s.max() - s.min()).alias(\"logbook_bingo_indexCount\"),\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader')) | (\n",
    "                            pl.col(\"fqid\") == \"reader.paper2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"reader_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader')) | (\n",
    "                        pl.col(\"fqid\") == \"reader.paper2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"reader_bingo_indexCount\"),\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals')) | (\n",
    "                            pl.col(\"fqid\") == \"journals.pic_2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"journals_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals')) | (\n",
    "                        pl.col(\"fqid\") == \"journals.pic_2.bingo\")).apply(lambda s: s.max() - s.min()).alias(\n",
    "                    \"journals_bingo_indexCount\"),\n",
    "            ]\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "        if grp == '13-22':\n",
    "            aggs = [\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"reader_flag_duration\"),\n",
    "                pl.col(\"index\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'reader_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"reader_flag_indexCount\"),\n",
    "                pl.col(\"elapsed_time\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"journals_flag.pic_0.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"journalsFlag_bingo_duration\"),\n",
    "                pl.col(\"index\").filter(\n",
    "                    ((pl.col(\"event_name\") == 'navigate_click') & (pl.col(\"fqid\") == 'journals_flag')) | (\n",
    "                            pl.col(\"fqid\") == \"journals_flag.pic_0.bingo\")).apply(\n",
    "                    lambda s: s.max() - s.min() if s.len() > 0 else 0).alias(\"journalsFlag_bingo_indexCount\")\n",
    "            ]\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "    return df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = feature_engineer(df1, grp='0-4', use_extra=True, feature_suffix='')\n",
    "df2 = feature_engineer(df2, grp='5-12', use_extra=True, feature_suffix='')\n",
    "df3 = feature_engineer(df3, grp='13-22', use_extra=True, feature_suffix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_feature(train):\n",
    "    train[\"year\"] = train[\"session_id\"].apply(lambda x: int(str(x)[:2])).astype(np.uint8)\n",
    "    train[\"month\"] = train[\"session_id\"].apply(lambda x: int(str(x)[2:4])+1).astype(np.uint8)\n",
    "    train[\"day\"] = train[\"session_id\"].apply(lambda x: int(str(x)[4:6])).astype(np.uint8)\n",
    "    train[\"hour\"] = train[\"session_id\"].apply(lambda x: int(str(x)[6:8])).astype(np.uint8)\n",
    "    train[\"minute\"] = train[\"session_id\"].apply(lambda x: int(str(x)[8:10])).astype(np.uint8)\n",
    "    train[\"second\"] = train[\"session_id\"].apply(lambda x: int(str(x)[10:12])).astype(np.uint8)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23562, 1873) (23562, 1879) (23562, 1877)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape, df2.shape, df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = time_feature(df1)\n",
    "df2 = time_feature(df2)\n",
    "df3 = time_feature(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1873/1873 [00:00<00:00, 3017.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1879/1879 [00:00<00:00, 2863.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1877/1877 [00:00<00:00, 2771.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339 1059 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "null1 = df1.isnull().sum().sort_values(ascending=False)/len(df1)\n",
    "null2 = df2.isnull().sum().sort_values(ascending=False)/len(df2)\n",
    "null3 = df3.isnull().sum().sort_values(ascending=False)/len(df3)\n",
    "\n",
    "drop1 = list(null1[null1 > 0.9].index)\n",
    "drop2 = list(null2[null2 > 0.9].index)\n",
    "drop3 = list(null3[null3 > 0.9].index)\n",
    "\n",
    "for col in tqdm(df1.columns):\n",
    "    if df1[col].nunique() == 1:\n",
    "#         print(col)\n",
    "        drop1.append(col)\n",
    "for col in tqdm(df2.columns):\n",
    "    if df2[col].nunique() == 1:\n",
    "#         print(col)\n",
    "        drop2.append(col)\n",
    "for col in tqdm(df3.columns):\n",
    "    if df3[col].nunique() == 1:\n",
    "#         print(col)\n",
    "        drop3.append(col)\n",
    "print(len(drop1), len(drop2), len(drop3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will train with 533 820 1004 features\n",
      "We will train with 23562 users info\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.set_index('session_id')\n",
    "df2 = df2.set_index('session_id')\n",
    "df3 = df3.set_index('session_id')\n",
    "\n",
    "FEATURES1 = [c for c in df1.columns if c not in drop1+['level_group']]\n",
    "FEATURES2 = [c for c in df2.columns if c not in drop2+['level_group']]\n",
    "FEATURES3 = [c for c in df3.columns if c not in drop3+['level_group']]\n",
    "print('We will train with', len(FEATURES1), len(FEATURES2), len(FEATURES3), 'features')\n",
    "ALL_USERS = df1.index.unique()\n",
    "print('We will train with', len(ALL_USERS), 'users info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "Length of label=424116 and length of data=0 is different.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-c8a64254645c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFEATURES1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m23562\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'correct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    615\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiving\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mof\u001b[0m \u001b[0mEmbedding\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmain\u001b[0m \u001b[1;34m'data'\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[1;36m2\u001b[0m\u001b[0md\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlists\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarrays\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_data_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0melements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m             \u001b[0mIf\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[1;36m2\u001b[0m\u001b[0md\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlists\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarrays\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_data_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0melements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m                 \u001b[0mDict\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecified\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'embedding_features'\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_weight_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1087\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_group_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_check_label_shape\u001b[1;34m(self, label, samples_count)\u001b[0m\n\u001b[0;32m    730\u001b[0m                             \u001b[1;34m\" when 'data' parameter has FeaturesData type\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                         )\n\u001b[1;32m--> 732\u001b[1;33m                 \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'f'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcat_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m                         raise CatBoostError(\n",
      "\u001b[1;31mCatBoostError\u001b[0m: Length of label=424116 and length of data=0 is different."
     ]
    }
   ],
   "source": [
    "Pool(df[FEATURES1].astype('float32').loc[:23562], targets['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "papermill": {
     "duration": 4397.140879,
     "end_time": "2023-03-10T13:13:02.900325",
     "exception": false,
     "start_time": "2023-03-10T11:59:45.759446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "question1, with533features\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/18 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-d4bb8c432029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mtrain_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'correct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mvalid_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'correct'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    615\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiving\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mof\u001b[0m \u001b[0mEmbedding\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmain\u001b[0m \u001b[1;34m'data'\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[1;36m2\u001b[0m\u001b[0md\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlists\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarrays\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_data_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0melements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m             \u001b[0mIf\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[1;36m2\u001b[0m\u001b[0md\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlists\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarrays\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_data_size\u001b[0m \u001b[0mx\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0melements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m                 \u001b[0mDict\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecified\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'embedding_features'\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[0;32m   1131\u001b[0m             \u001b[0mOutput\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \"\"\"\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_quantized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pool is not quantized'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5991\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "feature_importance_df = pd.DataFrame()\n",
    "models = {}\n",
    "results = [[[], []] for _ in range(18)]\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "oof_cat = pd.DataFrame(data=np.zeros((len(ALL_USERS),18)), index=ALL_USERS, columns=[f'meta_{i}' for i in range(1, 19)])\n",
    "for q in tqdm(range(1, 19)):\n",
    "    if q <= 3:\n",
    "        grp = '0-4'\n",
    "        df = df1\n",
    "        FEATURES = FEATURES1\n",
    "    elif q <= 13:\n",
    "        grp = '5-12'\n",
    "        df = df2\n",
    "        FEATURES = FEATURES2\n",
    "    elif q <= 22:\n",
    "        grp = '13-22'\n",
    "        df = df3\n",
    "        FEATURES = FEATURES3\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f'question{q}, with{len(FEATURES)}features')\n",
    "    print('#'*25)\n",
    "\n",
    "    cat_params = {\n",
    "        'iterations': 1000,\n",
    "        'early_stopping_rounds': 90,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.02,\n",
    "        'loss_function': \"Logloss\",\n",
    "        'random_seed': 222222,\n",
    "        'metric_period': 1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bylevel': 0.4,\n",
    "        'verbose': 0,\n",
    "        'l2_leaf_reg': 20,\n",
    "    }\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf.split(X=df, groups=df.index)):\n",
    "        # TRAIN DATA\n",
    "        train_x = df.iloc[train_idx]\n",
    "        train_users = train_x.index.values\n",
    "        train_y = targets.loc[targets.q == q].set_index('session').loc[train_users]\n",
    "\n",
    "        # VALID DATA\n",
    "        valid_x = df.iloc[valid_idx]\n",
    "        valid_users = valid_x.index.values\n",
    "        valid_y = targets.loc[targets.q == q].set_index('session').loc[valid_users]\n",
    "\n",
    "        \n",
    "        train_pool = Pool(train_x[FEATURES].astype('float32'), train_y['correct'])\n",
    "        valid_pool = Pool(valid_x[FEATURES].astype('float32'), valid_y['correct'])\n",
    "\n",
    "\n",
    "        model = CatBoostClassifier(**cat_params)\n",
    "        model = model.fit(train_pool, eval_set=valid_pool)\n",
    "\n",
    "        y = valid_pool.get_label()\n",
    "        y_hat = model.predict_proba(valid_pool)[:,1]\n",
    "        models[(fold, q)] = model\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = FEATURES\n",
    "        fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        results[q - 1][0].append(y)\n",
    "        results[q - 1][1].append(y_hat)\n",
    "    feature_importance_df = feature_importance_df.groupby(['feature'])['importance'].agg(['mean']).sort_values(by='mean', ascending=False)\n",
    "    display(feature_importance_df.head(10))\n",
    "results = [[np.concatenate(_) for _ in _] for _ in results]\n",
    "\n",
    "\n",
    "for (fold,q), model in models.items():\n",
    "    model.save_model(f'fold{fold}_q{q}.cbm')\n",
    "\n",
    "true = pd.DataFrame(np.stack([_[0] for _ in results]).T)\n",
    "oof = pd.DataFrame(np.stack([_[1] for _ in results]).T)\n",
    "\n",
    "scores = []; thresholds = []\n",
    "best_socre = 0; best_threshold = 0\n",
    "\n",
    "for threshold in np.arange(0.5, 0.7, 0.01):\n",
    "    preds = (oof.values.reshape(-1) > threshold).astype('int')\n",
    "    m = f1_score(true.values.reshape(-1), preds, average='macro')\n",
    "    scores.append(m)\n",
    "    thresholds.append(threshold)\n",
    "    if m > best_socre:\n",
    "        best_socre = m\n",
    "        best_threshold = threshold\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(thresholds, scores, '-o', color='blue')\n",
    "plt.scatter([best_threshold], [best_socre], color='blue')\n",
    "plt.xlabel(\"Threshold\", size=14)\n",
    "plt.ylabel(\"Validation F1 Score\",size=14)\n",
    "plt.title(f'Threshold vs. F1_Score with Best F1_Score={best_socre:.3f} at Best Threshold = {best_threshold:.3}', size=18)\n",
    "plt.show()\n",
    "\n",
    "print(f'When using optimal threshold = {best_threshold:.2f}...')\n",
    "for k in range(18):\n",
    "    m = f1_score(true[k].values, (oof[k].values > best_threshold).astype('int'), average = 'macro')\n",
    "    print(f'Q{k}: F1 =',m)\n",
    "m = f1_score(true.values.reshape(-1), (oof.values > best_threshold).reshape(-1).astype('int'), average = 'macro')\n",
    "print('==> Overall F1 =', m)\n",
    "print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.026201,
     "end_time": "2023-03-10T13:13:02.939877",
     "exception": false,
     "start_time": "2023-03-10T13:13:02.913676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "importance_dict = {}\n",
    "for t in range(1, 19):\n",
    "    if t<=3: \n",
    "        importance_dict[str(t)] = FEATURES1\n",
    "    elif t<=13: \n",
    "        importance_dict[str(t)] = FEATURES2\n",
    "    elif t<=22:\n",
    "        importance_dict[str(t)] = FEATURES3\n",
    "\n",
    "f_save = open('importance_dict.pkl', 'wb')\n",
    "pickle.dump(importance_dict, f_save)\n",
    "f_save.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4493.43494,
   "end_time": "2023-03-10T13:13:04.092855",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-10T11:58:10.657915",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
